## 机器规划
主机名称	IP	            用途          	实例规格	            镜像	                部署组件
log1	192.168.12.236	日志收集引擎	2核2GB以上、	           CentOS-7.3 minimal	   logstash、zookeeper
log2	192.168.12.237	日志通道机	2核2GB以上、40G硬盘以上	    CentOS-7.3 minimal	    kafka、zookeeper
log3	192.168.12.238	日志通道机	2核2GB以上、40G硬盘以上	    CentOS-7.3 minimal	    kafka、zookeeper

## 组件与版本选择
jdk-8u144-linux-x64.rpm	Jdk 1.8
logstash-5.5.2.tar.gz	logstash	Logstash 从kafka中读取消息
kafka_2.12-0.11.0.0.tgz	Kafka	
zookeeper-3.4.9.tar.gz	zookeeper	Zookeeper 管理kafka集群

## 组件下载地址
Jdk 1.8  ：	http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
logstash-5.5.2.rpm ：	https://artifacts.elastic.co/downloads/logstash/logstash-5.5.2.rpm
kafka_2.11-0.11.0.0.tgz ：	http://www-eu.apache.org/dist/kafka/0.11.0.0/kafka_2.12-0.11.0.0.tgz
zookeeper-3.4.9.tar.gz ：	http://www-eu.apache.org/dist/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz

scala2.11 

## 基础组件安装 @all
yum install -y lrzsz vim wget

## 上传或者下载文件
mkdir -p /data/pack
cd /data/pack

## jvm安装 @all
yum localinstall jdk-8u144-linux-x64.rpm –y

## 防火墙
firewall-cmd --zone=public --add-port=12888/tcp --permanent
firewall-cmd --zone=public --add-port=13888/tcp --permanent
firewall-cmd --zone=public --add-port=12181/tcp --permanent
firewall-cmd --zone=public --add-port=19092/tcp --permanent
firewall-cmd --reload

## zookeeper 
cd /data/pack
tar -zxvf zookeeper-3.4.9.tar.gz 
mv zookeeper-3.4.9 /usr/zookeeper
cd /usr/zookeeper/conf 
mkdir -p /data/zookeeper/zkdata
mkdir -p /data/zookeeper/zkdatalog
cp zoo_sample.cfg zoo.cfg
vim zoo.cfg

#修改配置如下
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/data/zookeeper/zkdata
dataLogDir=/data/zookeeper/zkdatalog
clientPort=12181
server.1=192.168.255.137:12888:13888
server.2=192.168.255.138:12888:13888
server.3=192.168.255.139:12888:13888 
#根据需要修改服务器ip


#创建Zookeeper所需要的myid文件,三台服务器分别操作。
#server1
echo "1" > /data/zookeeper/zkdata/myid
#server2
echo "2" > /data/zookeeper/zkdata/myid
#server3
echo "3" > /data/zookeeper/zkdata/myid

#设置Zookeeper开机启动
vim /usr/lib/systemd/system/zookeeper.service
#新增以下内容
[Unit]
Description=zookeeper.service
After=network.target

[Service]
Type=forking
Environment=/usr/zookeeper/
ExecStart=/usr/zookeeper/bin/zkServer.sh start
ExecStop=/usr/zookeeper/bin/zkServer.sh stop
ExecReload=/usr/zookeeper/bin/zkServer.sh restart

[Install]
WantedBy=multi-user.target

#保存退出
:wq

systemctl daemon-reload
systemctl enable zookeeper.service

## 启动所有zookeeper
systemctl start zookeeper

#验证
/usr/zookeeper/bin/zkServer.sh status
#其中有一台Mode显示leader另外两台显示follower则表示Zookeeper配置正确


## kafka安装
#Kafka 软件环境
1.	log2和log3两台服务器
2.	Jdk 1.8
3.	已经搭建好的zookeeper集群
4.	软件版本kafka_2.12-0.11.0.0.tgz
#创建目录
cd /data/pack
mkdir -p /data/kafka/kafkalogs

#解压软件
tar -zxvf kafka_2.12-0.11.0.0.tgz
mv kafka_2.12-0.11.0.0 /usr/kafka

## kafka配置
日志存放时间预计：72 h
日志大小预计：4G
vim /usr/kafka/config/server.properties
`
broker.id=0 #每台服务器的broker.id都不能相同
log.dirs=/data/kafka/kafkalogs
listeners=PLAINTEXT://xxx.xxx.xxx.xxx(本机ip):19092
#填写当前服务器ip
advertised.listeners=PLAINTEXT://xxx.xxx.xxx.xxx(本机ip):19092
port=19092

num.recovery.threads.per.data.dir=2

offsets.topic.replication.factor=2
transaction.state.log.replication.factor=2
transaction.state.log.min.isr=2

quota.producer.default=10485760
quota.consumer.default=10485760

#启用删除策略
log.cleanup.policy=delete
log.retention.hours=72
log.retention.bytes=4294967296

message.max.byte=10485760
default.replication.factor=2
replica.fetch.max.bytes=10485760


#设置zookeeper的连接端口
zookeeper.connect=192.168.255.139:12181,192.168.255.140:12181,192.168.255.141:12181

`

## 配置kafka开机启动
vim /usr/lib/systemd/system/kafka.service
`
#新增以下内容
[Unit]
Description=Apache Kafka server (broker)
Documentation=http://kafka.apache.org/documentation.html
After=network.target zookeeper.service

[Service]
Type=forking
Environment=/usr/kafka/
ExecStart=/usr/kafka/bin/kafka-server-start.sh -daemon /usr/kafka/config/server.properties
ExecStop=/usr/kafka/bin/kafka-server-stop.sh

[Install]
WantedBy=multi-user.target

`

#保存退出
:wq

systemctl daemon-reload
systemctl enable kafka.service


#Kafka 服务启动验证
systemctl start kafka.service

查看服务是否启动
[root@dypan139 logs]# jps
2196 QuorumPeerMain
2600 Jps
2539 Kafka
显示kafka则表示kafka已运行


## 创建topic  logHDFS 及 logCenter
/usr/kafka/bin/kafka-topics.sh --create --zookeeper 192.168.12.236:12181 --replication-factor 2 --partitions 2 --topic logHDFS
查看是否创建成功
/usr/kafka/bin/kafka-topics.sh --list --zookeeper 192.168.12.236:12181


## logstash安装
cd /data/pack
yum localinstall logstash-5.5.2.rpm -y 
#安装时注意有没有报错，这里要求java的路径为 /usr/bin/java 如果没有，那么请ln -s /usr/java/jdk1.8/bin/java /usr/bin/java
#yum -y remove logstash

systemctl daemon-reload
systemctl enable logstash.service

mkdir -p /data/logstash
chmod 777 -R /data/logstash
#新增kafka配置文件
vim /etc/logstash/conf.d/logCenter.conf


#以下是本地文件方式存储
`
#新增以下内容，boostrap_servers需要改成对应服务器地址

input {
        kafka {
                bootstrap_servers => ["192.168.112.180:19092,192.168.112.181:19092"]
                topics => "logCenter"
                group_id=> "default"
                codec => "json"
                max_partition_fetch_bytes => "10485760"
        }
}

filter {
        grok {
            match => ["source","%{GREEDYDATA}[/\\]%{GREEDYDATA:filename}$"]
        }
        mutate {
                add_field => {"logpath" => "%{[fields][system_id]}"}
        }
}

output {
        #stdout {}
        file {
                path => "/data/logstash/%{logpath}/%{+yyyy_MM_dd}/%{filename}"
        }
}


`
#以下是hdfs方式存储
vim /etc/logstash/conf.d/logHDFS.conf
`
input {
	kafka {
		bootstrap_servers => ["192.168.12.6:19092,192.168.12.6:19093"]
		topics => "logHDFS"
        group_id=> "default"
		codec => "json"
		max_partition_fetch_bytes => "10485760"
	}
}

filter {
	grok {
            match => ["source","%{GREEDYDATA}[/\\]%{GREEDYDATA:filename}$"]
        }
	mutate {
		add_field => {"logpath" => "%{[fields][system_id]}"}
	}
}

output {
	stdout {}
    webhdfs {
        host => "192.168.112.192"
        port => 50070
        path => "/data/logHDFS/%{logpath}/%{+yyyy-MM-dd}.log"
        user => "hdfs"
		codec => "json"
    }
}

`

## 性能优化及参数配置
vim /etc/logstash/logstash.yml

pipeline.workers: 24
pipeline.output.workers: 24
pipeline.batch.size: 10000
pipeline.batch.delay: 10

具体的workers/output.workers数量建议等于CPU数，batch.size/batch.delay根据实际的数据量逐渐增大来测试最优值

#启动及验证
systemctl start logstash.service
ps -ef | grep logstash
